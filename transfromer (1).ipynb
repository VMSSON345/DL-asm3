{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7fdca6",
      "metadata": {
        "id": "1f7fdca6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import sentencepiece as spm\n",
        "import wandb\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09dfd588",
      "metadata": {
        "id": "09dfd588"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 1. Embedding + Positional Encoding\n",
        "# ======================\n",
        "\n",
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=200):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i]   = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec5357a",
      "metadata": {
        "id": "dec5357a"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 2. Multi-head Attention\n",
        "# ======================\n",
        "\n",
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "    d_k = q.size(-1)\n",
        "\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout:\n",
        "        scores = dropout(scores)\n",
        "\n",
        "    return torch.matmul(scores, v)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        bs = q.size(0)\n",
        "\n",
        "        # linear projection + split into heads\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        # apply attention\n",
        "        scores = attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        # concat heads\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
        "\n",
        "        # output projection\n",
        "        return self.out(concat)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514d7ca6",
      "metadata": {
        "id": "514d7ca6"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 3. Feed Forward Network\n",
        "# ======================\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        return self.linear_2(x)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 4. Normalization Layer\n",
        "# ======================\n",
        "\n",
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.size = d_model\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias  = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.mean(-1, keepdim=True)\n",
        "        std  = x.std(-1, keepdim=True)\n",
        "        return self.alpha * (x - norm) / (std + self.eps) + self.bias\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 5. Encoder Layer\n",
        "# ======================\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "\n",
        "        self.attention = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attention(x2, x2, x2, mask))\n",
        "\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 6. Decoder Layer\n",
        "# ======================\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "\n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x  = x + self.dropout_1(self.attn_1(x2, x2, x2, tgt_mask))\n",
        "\n",
        "        x2 = self.norm_2(x)\n",
        "        x  = x + self.dropout_2(self.attn_2(x2, enc_out, enc_out, src_mask))\n",
        "\n",
        "        x2 = self.norm_3(x)\n",
        "        x  = x + self.dropout_3(self.ff(x2))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 7. Full Encoder\n",
        "# ======================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, heads, dropout) for _ in range(N)\n",
        "        ])\n",
        "\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 8. Full Decoder\n",
        "# ======================\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, heads, dropout) for _ in range(N)\n",
        "        ])\n",
        "\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, tgt, enc_out, src_mask, tgt_mask):\n",
        "        x = self.embed(tgt)\n",
        "        x = self.pe(x)\n",
        "\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, enc_out, src_mask, tgt_mask)\n",
        "\n",
        "        return self.norm(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054e5a7b",
      "metadata": {
        "id": "054e5a7b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ======================\n",
        "# 9. Seq2Seq Transformer\n",
        "# ======================\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=256, N=6, heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        e = self.encoder(src, src_mask)\n",
        "        d = self.decoder(tgt, e, src_mask, tgt_mask)\n",
        "        return self.out(d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c0af80",
      "metadata": {
        "id": "27c0af80"
      },
      "source": [
        "### SENTENCEPIECE TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734282f6",
      "metadata": {
        "id": "734282f6"
      },
      "outputs": [],
      "source": [
        "class SentencePieceTokenizer:\n",
        "    def __init__(self, model_prefix, vocab_size=8000):\n",
        "        self.model = None\n",
        "        self.model_prefix = model_prefix\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def train(self, texts):\n",
        "        with open(f\"{self.model_prefix}.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "            for t in texts:\n",
        "                f.write(t.strip() + \"\\n\")\n",
        "\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=f\"{self.model_prefix}.txt\",\n",
        "            model_prefix=self.model_prefix,\n",
        "            vocab_size=self.vocab_size,\n",
        "            character_coverage=1.0,\n",
        "            model_type=\"bpe\",\n",
        "            bos_id=1,\n",
        "            eos_id=2,\n",
        "            pad_id=0,\n",
        "            unk_id=3\n",
        "        )\n",
        "        self.model = spm.SentencePieceProcessor()\n",
        "        self.model.load(f\"{self.model_prefix}.model\")\n",
        "\n",
        "    def encode(self, text, max_len=100):\n",
        "        ids = self.model.encode(text, out_type=int)\n",
        "        ids = ids[:max_len]\n",
        "        return [1] + ids + [2]   # BOS=1, EOS=2\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self.model.decode(ids)\n",
        "\n",
        "    def vocab_size_(self):\n",
        "        return self.model.get_piece_size()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dea8820",
      "metadata": {
        "id": "3dea8820"
      },
      "source": [
        "### DATASET + COLLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccde674e",
      "metadata": {
        "id": "ccde674e"
      },
      "outputs": [],
      "source": [
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, src_tok, tgt_tok, max_len=100):\n",
        "        self.src = src_texts\n",
        "        self.tgt = tgt_texts\n",
        "        self.src_tok = src_tok\n",
        "        self.tgt_tok = tgt_tok\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_ids = self.src_tok.encode(self.src[idx], self.max_len)\n",
        "        tgt_ids = self.tgt_tok.encode(self.tgt[idx], self.max_len)\n",
        "        return torch.LongTensor(src_ids), torch.LongTensor(tgt_ids)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src, tgt = zip(*batch)\n",
        "    src = nn.utils.rnn.pad_sequence(src, batch_first=True, padding_value=0)\n",
        "    tgt = nn.utils.rnn.pad_sequence(tgt, batch_first=True, padding_value=0)\n",
        "    return src, tgt\n",
        "\n",
        "def make_src_mask(src):\n",
        "    return (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "def make_tgt_mask(tgt):\n",
        "    T = tgt.size(1)\n",
        "    pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
        "    seq_mask = torch.tril(torch.ones((T, T), device=tgt.device)).bool()\n",
        "    return pad_mask & seq_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eae2373",
      "metadata": {
        "id": "0eae2373"
      },
      "source": [
        "### TRAINING + PPL + W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f63f91",
      "metadata": {
        "id": "58f63f91"
      },
      "outputs": [],
      "source": [
        "from zmq import device\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_in = tgt[:, :-1]\n",
        "            tgt_out = tgt[:, 1:]\n",
        "\n",
        "            src_mask = make_src_mask(src)\n",
        "            tgt_mask = make_tgt_mask(tgt_in)\n",
        "\n",
        "            logits = model(src, tgt_in, src_mask, tgt_mask)\n",
        "            logits = logits.reshape(-1, logits.size(-1))\n",
        "            tgt_out = tgt_out.reshape(-1)\n",
        "\n",
        "            loss = criterion(logits, tgt_out)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl\n",
        "\n",
        "\n",
        "def train_pipeline(train_src, train_tgt, val_src, val_tgt,\n",
        "                   vocab_size=8000, batch_size=32, epochs=20):\n",
        "\n",
        "    # ====== WANDB ======\n",
        "    wandb.init(project=\"transformer-nmt\", config={\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"d_model\": 256,\n",
        "    \"heads\": 8,\n",
        "    \"layers\": 6,\n",
        "    \"epochs\": epochs,\n",
        "    \"batch_size\": batch_size\n",
        "    })\n",
        "\n",
        "\n",
        "    # ====== TOKENIZER SP ======\n",
        "    src_tok = SentencePieceTokenizer(\"sp_src\", vocab_size)\n",
        "    tgt_tok = SentencePieceTokenizer(\"sp_tgt\", vocab_size)\n",
        "    src_tok.train(train_src)\n",
        "    tgt_tok.train(train_tgt)\n",
        "\n",
        "    # ====== DATASET ======\n",
        "    train_ds = NMTDataset(train_src, train_tgt, src_tok, tgt_tok)\n",
        "    val_ds   = NMTDataset(val_src,   val_tgt,   src_tok, tgt_tok)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size,\n",
        "                              shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "    # ====== MODEL ======\n",
        "    model = Transformer(\n",
        "    src_tok.vocab_size_(),\n",
        "    tgt_tok.vocab_size_(),\n",
        "    d_model=256,\n",
        "    N=6,\n",
        "    heads=8\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    # ====== TRAIN LOOP ======\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for src, tgt in train_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_in = tgt[:, :-1]\n",
        "            tgt_out = tgt[:, 1:]\n",
        "\n",
        "            src_mask = make_src_mask(src)\n",
        "            tgt_mask = make_tgt_mask(tgt_in)\n",
        "\n",
        "            logits = model(src, tgt_in, src_mask, tgt_mask)\n",
        "            loss = criterion(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                tgt_out.reshape(-1)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # ====== VALIDATION ======\n",
        "        val_loss, ppl = validate(model, val_loader, criterion)\n",
        "\n",
        "        # ====== LOG W&B ======\n",
        "        wandb.log({\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"perplexity\": ppl\n",
        "        })\n",
        "\n",
        "        print(f\"[Epoch {ep+1}] Train={train_loss:.3f}  Val={val_loss:.3f}  PPL={ppl:.2f}\")\n",
        "\n",
        "        # ====== SAVE BEST ======\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "\n",
        "    return model, src_tok, tgt_tok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_iwslt15_text(path):\n",
        "    train_en = open(os.path.join(path, \"train.en\"), encoding=\"utf8\").read().splitlines()\n",
        "    train_vi = open(os.path.join(path, \"train.vi\"), encoding=\"utf8\").read().splitlines()\n",
        "\n",
        "    dev_en = open(os.path.join(path, \"tst2012.en\"), encoding=\"utf8\").read().splitlines()\n",
        "    dev_vi = open(os.path.join(path, \"tst2012.vi\"), encoding=\"utf8\").read().splitlines()\n",
        "\n",
        "    test_en = open(os.path.join(path, \"tst2013.en\"), encoding=\"utf8\").read().splitlines()\n",
        "    test_vi = open(os.path.join(path, \"tst2013.vi\"), encoding=\"utf8\").read().splitlines()\n",
        "\n",
        "    print(\"Loaded IWSLT15:\")\n",
        "    print(\" - Train:\", len(train_en))\n",
        "    print(\" - Dev  :\", len(dev_en))\n",
        "    print(\" - Test :\", len(test_en))\n",
        "\n",
        "    return (train_en, train_vi), (dev_en, dev_vi), (test_en, test_vi)\n"
      ],
      "metadata": {
        "id": "oVXiuU7u0xpF"
      },
      "id": "oVXiuU7u0xpF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed66660b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed66660b",
        "outputId": "e597a8d4-24e1-418a-d3cd-a5e710d5b0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(train_en, train_vi), (dev_en, dev_vi), _ = load_iwslt15_text(\"/content/drive/MyDrive/Assignment_nlp/data\")\n",
        "\n",
        "model, sp_src, sp_tgt = train_pipeline(\n",
        "    train_en, train_vi,\n",
        "    dev_en, dev_vi,\n",
        "    vocab_size=8000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "4mcoWhGx05mH",
        "outputId": "7f945b1a-10da-4774-c6ff-4abb558fad9e"
      },
      "id": "4mcoWhGx05mH",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded IWSLT15:\n",
            " - Train: 133317\n",
            " - Dev  : 1553\n",
            " - Test : 1268\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m23020424\u001b[0m (\u001b[33mvuminhson-vietnam-national-university-hanoi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251123_040700-a3rz3esc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vuminhson-vietnam-national-university-hanoi/transformer-nmt/runs/a3rz3esc' target=\"_blank\">lemon-aardvark-1</a></strong> to <a href='https://wandb.ai/vuminhson-vietnam-national-university-hanoi/transformer-nmt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/vuminhson-vietnam-national-university-hanoi/transformer-nmt' target=\"_blank\">https://wandb.ai/vuminhson-vietnam-national-university-hanoi/transformer-nmt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/vuminhson-vietnam-national-university-hanoi/transformer-nmt/runs/a3rz3esc' target=\"_blank\">https://wandb.ai/vuminhson-vietnam-national-university-hanoi/transformer-nmt/runs/a3rz3esc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train=4.546  Val=3.812  PPL=45.26\n",
            "[Epoch 2] Train=3.562  Val=3.336  PPL=28.11\n",
            "[Epoch 3] Train=3.144  Val=3.068  PPL=21.49\n",
            "[Epoch 4] Train=2.858  Val=2.906  PPL=18.28\n",
            "[Epoch 5] Train=2.642  Val=2.779  PPL=16.10\n",
            "[Epoch 6] Train=2.474  Val=2.683  PPL=14.63\n",
            "[Epoch 7] Train=2.339  Val=2.616  PPL=13.68\n",
            "[Epoch 8] Train=2.229  Val=2.602  PPL=13.50\n",
            "[Epoch 9] Train=2.135  Val=2.569  PPL=13.06\n",
            "[Epoch 10] Train=2.054  Val=2.543  PPL=12.72\n",
            "[Epoch 11] Train=1.982  Val=2.525  PPL=12.49\n",
            "[Epoch 12] Train=1.918  Val=2.519  PPL=12.42\n",
            "[Epoch 13] Train=1.859  Val=2.516  PPL=12.38\n",
            "[Epoch 14] Train=1.807  Val=2.513  PPL=12.34\n",
            "[Epoch 15] Train=1.758  Val=2.532  PPL=12.58\n",
            "[Epoch 16] Train=1.712  Val=2.530  PPL=12.55\n",
            "[Epoch 17] Train=1.669  Val=2.525  PPL=12.49\n",
            "[Epoch 18] Train=1.631  Val=2.542  PPL=12.71\n",
            "[Epoch 19] Train=1.594  Val=2.555  PPL=12.87\n",
            "[Epoch 20] Train=1.559  Val=2.579  PPL=13.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_en, train_vi), (dev_en, dev_vi), (test_en, test_vi) = load_iwslt15_text(\"/content/drive/MyDrive/Assignment_nlp/data\")\n",
        "\n",
        "test_ds = NMTDataset(test_en, test_vi, sp_src, sp_tgt)\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "test_loss, test_ppl = validate(model, test_loader, nn.CrossEntropyLoss(ignore_index=0))\n",
        "\n",
        "print(\"TEST LOSS:\", test_loss)\n",
        "print(\"TEST PPL:\", test_ppl)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g4zbztaMoNc",
        "outputId": "76064bce-4c69-4bb0-810a-43c13a4d582d"
      },
      "id": "7g4zbztaMoNc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded IWSLT15:\n",
            " - Train: 133317\n",
            " - Dev  : 1553\n",
            " - Test : 1268\n",
            "TEST LOSS: 2.4414097666740417\n",
            "TEST PPL: 11.489226459645373\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}